{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c100.local:8088/proxy/application_1596895008206_25693\n",
       "SparkContext available as 'sc' (version = 2.4.0-cdh6.3.2, master = yarn, app id = application_1596895008206_25693)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, IDF, StringIndexer, ChiSqSelector}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import scala.collection.mutable.ListBuffer\n",
       "import java.io.{File, PrintWriter}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, IDF, StringIndexer, ChiSqSelector}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.io.{File, PrintWriter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DEVSET: String = hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val DEVSET = \"hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load data into DataFrame and select relevant columns\n",
    "val reviewsDf = spark.read.json(DEVSET).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_ff3bfc3b02db\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the category has to be encoded as index for the ChiSqSelector to work\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_f039a03b4583\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the tokenizer creates unigrams from the given regex pattern\n",
    "// further case folding and removing of tokens with less than two characters is done\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"tokensRaw\")\n",
    "    .setPattern(\"[ \\t0123456789.!?,;:()\\\\[\\\\]{}\\\\-_\\\"'`~#&*%$\\\\\\\\/]+\")\n",
    "    .setToLowercase(true)\n",
    "    .setMinTokenLength(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWordsRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_36160cc47a7c\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove stop words which should be ignored for text classification\n",
    "val stopWordsRemover = new StopWordsRemover()\n",
    "    .setInputCol(\"tokensRaw\")\n",
    "    .setOutputCol(\"tokens\")\n",
    "    .setStopWords(Array(\"a\", \"aa\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"ain\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"aren\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\", \"away\", \"awfully\", \"b\", \"bb\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bibs\", \"book\", \"both\", \"brief\", \"but\", \"by\", \"c\", \"came\", \"can\", \"cannot\", \"cant\", \"car\", \"cause\", \"causes\", \"cd\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"course\", \"currently\", \"d\", \"definitely\", \"described\", \"despite\", \"did\", \"didn\", \"different\", \"do\", \"does\", \"doesn\", \"doing\", \"don\", \"done\", \"down\", \"downwards\", \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"game\", \"game\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn\", \"happens\", \"hardly\", \"has\", \"hasn\", \"have\", \"haven\", \"having\", \"he\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn\", \"it\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"keep\", \"keeps\", \"kept\", \"know\", \"known\", \"knows\", \"l\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"life\", \"like\", \"liked\", \"likely\", \"little\", \"ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\", \"mon\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"presumably\", \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \"should\", \"shouldn\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"t\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"ve\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\", \"wasn\", \"way\", \"we\", \"welcome\", \"well\", \"went\", \"were\", \"weren\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won\", \"wonder\", \"would\", \"wouldn\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_7c8dcae98a51\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the first part of TF-IDF is to calculate the term frequency \n",
    "// for this we use the CountVectorizer and not HashingTF althoug it is not as performant\n",
    "// because we need access to the vocabulary later\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(\"tokens\")\n",
    "    .setOutputCol(\"featuresRaw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idf: org.apache.spark.ml.feature.IDF = idf_ae58982393a2\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now the term frequency is multiplyed by the inverse document frequency using the IDF class\n",
    "val idf = new IDF()\n",
    "    .setInputCol(\"featuresRaw\")\n",
    "    .setOutputCol(\"featuresWeighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_5fd8fc74f524\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// last Chi-Squared feature selection is used to extract the top 4000 tokens overall\n",
    "val selector = new ChiSqSelector()\n",
    "    .setNumTopFeatures(4000)\n",
    "    .setLabelCol(\"label\")\n",
    "    .setFeaturesCol(\"featuresWeighted\")\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract top 4000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 1 more field]\n",
       "tokenizedDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 2 more fields]\n",
       "tokenizedFilteredDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 3 more fields]\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_7c8dcae98a51\n",
       "tfDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 4 more fields]\n",
       "tfIdfDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 5 more fields]\n",
       "cSqModel: org.apache.spark.ml.feature.ChiSqSelectorModel = chiSqSelector_5fd8fc74f524\n",
       "topFeaturesDf: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 6 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now we run all steps needed for the feature creation and selection\n",
    "val indexDf = indexer.fit(reviewsDf).transform(reviewsDf)\n",
    "val tokenizedDf = tokenizer.transform(indexDf)\n",
    "val tokenizedFilteredDf = stopWordsRemover.transform(tokenizedDf)\n",
    "val cvModel = countVectorizer.fit(tokenizedFilteredDf)\n",
    "val tfDf = cvModel.transform(tokenizedFilteredDf)\n",
    "val tfIdfDf = idf.fit(tfDf).transform(tfDf)\n",
    "val cSqModel = selector.fit(tfIdfDf)\n",
    "val topFeaturesDf = cSqModel.transform(tfIdfDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            category|          reviewText|class|           tokensRaw|              tokens|         featuresRaw|    featuresWeighted|            features|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...| 18.0|[this, was, gift,...|[gift, husband, m...|(96489,[2,3,4,10,...|(96489,[2,3,4,10,...|(4000,[2,3,4,10,1...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...| 18.0|[this, is, very, ...|[nice, spreader, ...|(96489,[0,1,4,25,...|(96489,[0,1,4,25,...|(4000,[0,1,4,25,4...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...| 18.0|[the, metal, base...|[metal, base, hos...|(96489,[7,13,34,2...|(96489,[7,13,34,2...|(4000,[7,13,189,3...|\n",
      "|Patio_Lawn_and_Garde|For the most part...| 18.0|[for, the, most, ...|[part, works, pre...|(96489,[1,4,7,12,...|(96489,[1,4,7,12,...|(4000,[1,4,7,12,1...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...| 18.0|[this, hose, is, ...|[hose, supposed, ...|(96489,[16,38,49,...|(96489,[16,38,49,...|(4000,[16,35,113,...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...| 18.0|[this, tool, work...|[tool, works, cut...|(96489,[0,4,7,11,...|(96489,[0,4,7,11,...|(4000,[0,4,7,11,1...|\n",
      "|Patio_Lawn_and_Garde|This product is a...| 18.0|[this, product, i...|[product, typical...|(96489,[6,22,72,1...|(96489,[6,22,72,1...|(4000,[6,22,124,1...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...| 18.0|[was, excited, to...|[excited, ditch, ...|(96489,[9,25,27,4...|(96489,[9,25,27,4...|(4000,[9,25,27,38...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...| 18.0|[purchased, the, ...|[purchased, leaf,...|(96489,[4,7,8,9,5...|(96489,[4,7,8,9,5...|(4000,[4,7,8,9,49...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...| 18.0|[never, used, man...|[manual, lawnmowe...|(96489,[9,11,48,9...|(96489,[9,11,48,9...|(4000,[9,11,45,90...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...| 18.0|[good, price, goo...|[good, price, goo...|(96489,[1,2,6,17,...|(96489,[1,2,6,17,...|(4000,[1,2,6,17,2...|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...| 18.0|[have, owned, the...|[owned, flowtron,...|(96489,[6,8,21,42...|(96489,[6,8,21,42...|(4000,[6,8,21,39,...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...| 18.0|[had, won, simila...|[similar, product...|(96489,[1,6,14,37...|(96489,[1,6,14,37...|(4000,[1,6,14,34,...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...| 18.0|[the, birds, ate,...|[birds, ate, blue...|(96489,[53,173,29...|(96489,[53,173,29...|(4000,[49,157,362...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...| 18.0|[bought, last, su...|[bought, summer, ...|(96489,[0,4,10,12...|(96489,[0,4,10,12...|(4000,[0,4,10,12,...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...| 18.0|[knew, had, mouse...|[knew, mouse, bas...|(96489,[11,33,34,...|(96489,[11,33,34,...|(4000,[11,31,66,9...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...| 18.0|[was, little, wor...|[worried, reading...|(96489,[1,19,142,...|(96489,[1,19,142,...|(4000,[1,19,132,1...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...| 18.0|[have, used, this...|[brand, long, tim...|(96489,[3,4,28,27...|(96489,[3,4,28,27...|(4000,[3,4,236,28...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...| 18.0|[actually, do, no...|[current, model, ...|(96489,[6,7,13,20...|(96489,[6,7,13,20...|(4000,[6,7,13,20,...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...| 18.0|[just, what, expe...|[expected, works,...|(96489,[0,22,39,5...|(96489,[0,22,39,5...|(4000,[0,22,36,51...|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// our final DataFrame contains all intermediate calculations as well as the final 4000 top features\n",
    "topFeaturesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabulary: Array[String] = Array(great, good, read, love, time, story, product, work, recommend, back, easy, make, bought, made, find, books, buy, price, put, reading, quality, people, works, quot, years, nice, characters, case, long, series, lot, found, author, day, bit, movie, feel, makes, thing, perfect, fit, end, set, loved, things, thought, album, music, small, hard, phone, give, fun, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, song, days, enjoy, songs, place, home, stars, short, film, writing, play, cover, top, full, fan, fine, color, side, order, wonderful, amazing, point, fact, reviews, o..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// as only indexes are saved in the features we need the original vocabulary to extract the tokens as strings\n",
    "val vocabulary = cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectedFeatures: Array[Int] = Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29, 31, 32, 33, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 159, 160, 161, 162, 163, 164, 165, 166, 168, 170, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 1..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// the ChiSqSelector model stores the 4000 top selected features\n",
    "val selectedFeatures = cSqModel.selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectedTokenList: scala.collection.mutable.ListBuffer[String] = ListBuffer(great, good, read, love, time, story, product, work, recommend, back, easy, make, bought, made, find, books, buy, price, put, reading, quality, people, works, quot, years, nice, characters, case, series, found, author, day, movie, feel, makes, thing, perfect, fit, end, set, loved, things, thought, album, music, small, phone, give, fun, year, world, size, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, enjoyed, problem, family, interesting, character, job, review, purchase, man, watch, song, enjoy, songs, place, home, short, film, writing, play, cover, top, full, fan, fine, color, side, order, wonderful, amazing, point, fact, ordered, stories, favori..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now we access and store the selected features from the vocabulary and sort the tokens\n",
    "var selectedTokenList = new ListBuffer[String]()\n",
    "for (feature <- selectedFeatures) {\n",
    "    selectedTokenList += vocabulary(feature)\n",
    "}\n",
    "var selectedTokens = selectedTokenList.toList.sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write tokens to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myfile: java.io.File = ./output_ds.txt\n",
       "pw: java.io.PrintWriter = java.io.PrintWriter@66385335\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create output file and print writer\n",
    "val myfile = new File(\"./output_ds.txt\" )\n",
    "val pw = new PrintWriter(myfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// write tokens to output file\n",
    "for (token <- selectedTokens) {\n",
    "    pw.write(token + \" \")\n",
    "}\n",
    "pw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_44fcffe7859e\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// as running everything seperately is tedious we can put all steps into a pipeline which performs all specified steps one after the other\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(indexer, tokenizer, stopWordsRemover, countVectorizer, idf, selector))\n",
    "val featureDF = pipeline.fit(df).transform(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
